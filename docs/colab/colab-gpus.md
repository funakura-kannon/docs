# NVIDIA T4
* **アーキテクチャ：** Turing (2018年)
* **VRAM：** 16 GB (GDDR6)
* **位置付け：** **低コスト・エントリーレベルの推論**
* **特徴：**
    * 発表から時間は経過しているが、主要クラウド（AWS, GCP, Azure）において、最も安価なGPUインスタンスとして広く利用可能である。
    * 消費電力が低く（70W）、汎用サーバーに搭載しやすい。
* **主な用途：**
    * 小〜中規模モデルのAI推論（画像分類、音声認識など）。
    * 開発・テスト環境。
    * **量子化**（INT8/4bit）を前提とした、小規模なLLMの推論。
* **弱点：**
    * VRAMが16GBのため、大規模なLLMの運用には不向き。
    * 旧世代アーキテクチャであり、最新のLLM高速化技術（FP8対応など）に非対応。

---
# NVIDIA L4
* **アーキテクチャ：** Ada Lovelace (2023年)
* **VRAM：** 24 GB (GDDR6X)
* **位置付け：** **高効率・次世代の推論**
* **特徴：**
    * **T4の直接的な後継**にあたる。
    * Turing世代から大幅に性能が向上しており、特にTransformerベースのモデル（LLM）や動画処理（AV1エンコード/デコード）に強い。
    * 最新のLLM推論フォーマットである**FP8**に対応し、高速かつ高効率な推論が可能。
* **主な用途：**
    * AI推論全般（T4よりも高速・高効率）。
    * LLM推論（T4ではメモリ不足になる中規模モデルにも対応）。
    * 高画質なビデオストリーミング、AIを活用した動画解析。
* **対T4：** VRAMが増え、性能（特にLLM）が大幅に向上した後継モデル。

---

# NVIDIA A100
* **アーキテクチャ：** Ampere (2020年)
* **VRAM：** 40 GB / 80 GB (HBM2e)
* **位置付け：** **AI学習（トレーニング）向けのハイエンドGPU**
* **特徴：**
    * **Ampere世代のフラッグシップ**であり、AIの「学習（トレーニング）」のために設計された高性能GPU。
    * GDDRメモリではなく、非常に高速な広帯域メモリ（HBM2e）を搭載し、膨大なデータを高速に処理できる。
    * グラフィックス処理用のRTコアは搭載しておらず、純粋な計算に特化している。
* **主な用途：**
    * **大規模AIモデルの学習**（LLM、画像生成AIなど）。
    * 大規模なハイパフォーマンスコンピューティング（HPC）、科学技術計算。
    * 非常に大規模なモデルの推論。
* **対T4/L4：** T4やL4が主に「推論（実行）」に使われるのに対し、A100は主に「学習（開発）」に使われるハイエンド帯のモデルである。（H100やB200など、これより上位の学習用GPUが存在する。）